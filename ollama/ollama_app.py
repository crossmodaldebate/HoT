import streamlit as st
import ollama
import os
import json
import time

def make_api_call(messages, max_tokens, is_final_answer=False):
    for attempt in range(3):
        try:
            response = ollama.chat(
                model="llama3.1:70b",
                messages=messages,
                options={"temperature":0.2, "max_length":max_tokens},
                format='json',
            )
            return json.loads(response['message']['content'])
        except Exception as e:
            if attempt == 2:
                if is_final_answer:
                    return {"title": "Erro", "content": f"Falha ao gerar a resposta final ap√≥s 3 tentativas. Erro: {str(e)}"}
                else:
                    return {"title": "Erro", "content": f"Falha ao gerar etapa ap√≥s 3 tentativas. Erro: {str(e)}", "next_action": "resposta_final"}
            time.sleep(1)  # Espera por 1 segundo antes de tentar novamente

def generate_response(prompt):
    messages = [
        {"role": "system", "content": """Voc√™ √© um assistente de IA especialista que utiliza m√©todos heur√≠sticos e hermen√™uticos com uma arquitetura em hipergrafo. Para cada etapa, forne√ßa um t√≠tulo que descreva o objetivo da etapa e o conte√∫do explicativo. Siga a estrutura de um hipergrafo para decompor o problema em n√≥s e suas rela√ß√µes.

Exemplo de resposta JSON v√°lida:
json
{
    "title": "Identifica√ß√£o de N√≥s Principais",
    "content": "Para come√ßar a resolver este problema, precisamos identificar os n√≥s principais e suas rela√ß√µes dentro do hipergrafo. Isso envolve...",
    "next_action": "continuar"
}
"""},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": "Obrigado! Vou come√ßar a pensar passo a passo seguindo minhas instru√ß√µes, iniciando pela decomposi√ß√£o do problema em uma arquitetura de hipergrafo."}
    ]
    
    steps = []
    step_count = 1
    total_thinking_time = 0
    
    while True:
        start_time = time.time()
        step_data = make_api_call(messages, 300)
        end_time = time.time()
        thinking_time = end_time - start_time
        total_thinking_time += thinking_time
        
        steps.append((f"Etapa {step_count}: {step_data['title']}", step_data['content'], thinking_time))
        
        messages.append({"role": "assistant", "content": json.dumps(step_data)})
        
        if step_data['next_action'] == 'resposta_final' or step_count > 25:  # M√°ximo de 25 etapas para evitar tempo de pensamento infinito. Pode ser ajustado.
            break
        
        step_count += 1

        # Yield ap√≥s cada etapa para o Streamlit atualizar
        yield steps, None  # N√£o estamos retornando o tempo total at√© o final

    # Gerar resposta final
    messages.append({"role": "user", "content": "Por favor, forne√ßa a resposta final com base no seu racioc√≠nio acima."})
    
    start_time = time.time()
    final_data = make_api_call(messages, 200, is_final_answer=True)
    end_time = time.time()
    thinking_time = end_time - start_time
    total_thinking_time += thinking_time
    
    steps.append(("Resposta Final", final_data['content'], thinking_time))

    yield steps, total_thinking_time

def main():
    st.set_page_config(page_title="g1 prot√≥tipo", page_icon="üß†", layout="wide")
    
    st.title("g1: Usando Llama-3.1 70b no Ollama para criar cadeias de racioc√≠nio semelhantes ao o1 com Arquitetura de Hipergrafo")
    
    st.markdown("""
    Este √© um prot√≥tipo inicial de uso de m√©todos heur√≠sticos e hermen√™uticos com uma arquitetura em hipergrafo para melhorar a precis√£o de sa√≠da. N√£o √© perfeito e a precis√£o ainda n√£o foi formalmente avaliada. 

    Reposit√≥rio open source [aqui](https://github.com/bklieger-groq)
    """)
    
    # Entrada de texto para consulta do usu√°rio
    user_query = st.text_input("Digite sua consulta:", placeholder="ex: Quantos 'R's h√° na palavra morango?")
    
    if user_query:
        st.write("Gerando resposta...")
        
        # Criar elementos vazios para armazenar o texto gerado e o tempo total
        response_container = st.empty()
        time_container = st.empty()
        
        # Gerar e exibir a resposta
        for steps, total_thinking_time in generate_response(user_query):
            with response_container.container():
                for i, (title, content, thinking_time) in enumerate(steps):
                    if title.startswith("Resposta Final"):
                        st.markdown(f"### {title}")
                        st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
                    else:
                        with st.expander(title, expanded=True):
                            st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
            
            # Mostrar o tempo total apenas quando estiver dispon√≠vel no final
            if total_thinking_time is not None:
                time_container.markdown(f"Tempo total de pensamento: {total_thinking_time:.2f} segundos")

if _name_ == "_main_":
    main()
